{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55cb8577-30ab-46d8-be96-46c37ff641a1",
   "metadata": {},
   "source": [
    "### 下载ChatGLM-3    \n",
    "(大概5-10分钟,有些机子是阿里的，链接Modelscope特别快）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c996425-3f1f-43cc-893a-90bffe74d35b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n",
      "Cloning into 'chatglm3-6b'...\n",
      "remote: Enumerating objects: 106, done.\u001b[K\n",
      "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
      "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
      "remote: Total 106 (delta 45), reused 88 (delta 38), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (106/106), 41.14 KiB | 2.57 MiB/s, done.\n",
      "Resolving deltas: 100% (45/45), done.\n",
      "Filtering content: 100% (8/8), 11.63 GiB | 146.96 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git lfs install\n",
    "!cd /root/autodl-tmp/&&git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git\n",
    "!cp -f /root/tools/tokenization_chatglm.py /root/autodl-tmp/chatglm3-6b/ #修复tokenization代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716b231-f214-409e-8dd5-b88e82b691ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 音频数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f8a32-b1a8-402f-bb37-f3493e52a6b9",
   "metadata": {
    "tags": []
   },
   "source": [
    "将MP4处理成WAV,等待处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c4ad8d-f7b5-4419-8d6e-0e448c43c27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe12bbd7ec84614b620e8607dcfd53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "directory = '/root/autodl-tmp/DATA/INPUT'\n",
    "\n",
    "def extract_audio_to_wav(video_file,audio_filename):\n",
    "    extract_command = [\n",
    "        \"ffmpeg\",\"-loglevel\", \"quiet\", \"-y\", \"-i\", video_file, \n",
    "        \"-vn\", \"-acodec\", \"pcm_s16le\",  \n",
    "        \"-ar\", \"44100\", \"-ac\", \"2\",  \n",
    "        audio_filename\n",
    "    ]\n",
    "    subprocess.run(extract_command, check=True)\n",
    "\n",
    "\n",
    "i=1\n",
    "for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith(\".mp4\"):  \n",
    "        # print(f\"正在转换{filename}\")\n",
    "        filename=os.path.join(directory,filename)\n",
    "        extract_audio_to_wav(filename,os.path.join(directory,f\"origin_{i}.wav\"))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b73ff5-a71b-45de-a3a0-cd9428cced37",
   "metadata": {
    "tags": []
   },
   "source": [
    "将WAV切成2分钟小片，准备背景音处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e404026a-ad92-49ac-ac92-5ccf63e456b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bcc8d03f944901b17946a7e581d82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "input_directory = '/root/autodl-tmp/DATA/INPUT'\n",
    "output_directory = '/root/autodl-tmp/DATA/PRE_CUT'\n",
    "\n",
    "def slice_audio_fixed_intervals(input_audio_path, temporary_directory):\n",
    "    slice_duration = 2*60  # 设置2分钟一个切片\n",
    "    audio = AudioSegment.from_file(input_audio_path)\n",
    "    length = len(audio)\n",
    "    basename = os.path.splitext(os.path.basename(input_audio_path))[0]\n",
    "    start = 0\n",
    "    delay = 0\n",
    "    max_delay = 18 * 1000  # 最大不能延迟裁剪20秒\n",
    "\n",
    "    while start < length:\n",
    "        end = min(start + slice_duration * 1000 + delay, length)\n",
    "        chunk = audio[start:end]\n",
    "        loudness = chunk.dBFS\n",
    "        is_too_loud = loudness > 40\n",
    "\n",
    "        if is_too_loud and delay < max_delay:\n",
    "            delay += 3*1000  # 若遇到说话每次向后推迟3秒\n",
    "            continue\n",
    "\n",
    "        chunk_filename = f\"{basename}_slice_{start//1000}_{end//1000}.wav\"\n",
    "        chunk.export(os.path.join(temporary_directory, chunk_filename), format=\"wav\")\n",
    "        start = end\n",
    "        delay = 0\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for filename in tqdm(os.listdir(input_directory)):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        input_audio_path = os.path.join(input_directory, filename)\n",
    "        slice_audio_fixed_intervals(input_audio_path, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023b23c-49ac-4806-b454-91f8aec8c423",
   "metadata": {
    "tags": []
   },
   "source": [
    "将所有预切片进行背景音处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edafc075-8be3-4531-91dd-9be639319b02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tools.inference_subway import *\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Process, Queue, Lock, current_process,Event\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "def run_music_separator_main(worker_id,input_audio, output_folder,output_queue, inference_input_queue,lock):\n",
    "    options = {\n",
    "    \"input_audio\": [],\n",
    "    \"output_folder\": \"\",\n",
    "    \"use_cpu\": False,\n",
    "    \"overlap_demucs\": 0.6,\n",
    "    \"overlap_MDX\": 0,\n",
    "    \"overlap_MDXv3\": 20,\n",
    "    \"weight_MDXv3\": 8,\n",
    "    \"weight_VOCFT\": 3,\n",
    "    \"weight_HQ3\": 2,\n",
    "    \"single_onnx\": False,\n",
    "    \"chunk_size\": 1000000,\n",
    "    \"large_gpu\": True,\n",
    "    \"bigshifts\": 6,\n",
    "    \"vocals_only\": True,\n",
    "    \"output_format\": 'PCM_16',\n",
    "    \"lock\":lock,\n",
    "    \"output_queue\":output_queue, \n",
    "    \"inference_input_queue\":inference_input_queue,\n",
    "    \"worker_id\":worker_id,\n",
    "    }\n",
    "    options[\"input_audio\"] = [input_audio]\n",
    "    options[\"output_folder\"] = output_folder\n",
    "    run_music_separator(**options)\n",
    "# 推理进程函数\n",
    "def inference_process(inference_input_queue, inference_output_queues, lock,model_loaded_event):\n",
    "    ort1, ort2 = get_ort()  # 获取模型实例\n",
    "    print(\"初始化完毕\")\n",
    "    model_loaded_event.set()\n",
    "    while True:\n",
    "        task = inference_input_queue.get()\n",
    "        if task is None:  # 接收到退出信号\n",
    "            break\n",
    "        task_id, input_data, worker_id,infer_session_id = task['task_id'], task['input_data'], task['worker_id'],task['infer_session_id']\n",
    "\n",
    "        if infer_session_id == 1:\n",
    "            result = ort1.run(None, {'input':input_data})[0]\n",
    "        else:\n",
    "            result = ort2.run(None, {'input':input_data})[0]\n",
    "\n",
    "        inference_output_queue = inference_output_queues[worker_id]\n",
    "        inference_output_queue.put({'task_id': task_id, 'result': result})\n",
    "\n",
    "    # 发送退出信号到所有输出队列\n",
    "    for worker_id, output_queue in inference_output_queues.items():\n",
    "        output_queue.put(None)\n",
    "\n",
    "def progress_updater(progress_queue,pbar):\n",
    "    while True:\n",
    "        flag = progress_queue.get()  \n",
    "        if flag is None:\n",
    "            break\n",
    "        print(\" \", end='\\r') \n",
    "        pbar.update(1)  \n",
    "    pbar.close()\n",
    "        \n",
    "# 模型服务进程函数\n",
    "def model_server_task(worker_id, progress_queue, input_queue, output_queue, inference_input_queue, lock,model_loaded_event):\n",
    "    \n",
    "    print(f\"Model server {worker_id} started.\")\n",
    "    while True:\n",
    "        task = input_queue.get()\n",
    "        if task is None:  # 接收到退出信号\n",
    "            output_queue.put(None)\n",
    "            break\n",
    "        input_audio, output_folder = task\n",
    "       \n",
    "        run_music_separator_main(worker_id,input_audio, output_folder,output_queue, inference_input_queue,lock)\n",
    "        progress_queue.put(1) \n",
    "    print(f\"Model server {worker_id} exiting.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lock = Lock()\n",
    "    model_loaded_event = Event()\n",
    "    num_model_servers = 1\n",
    "    inference_input_queue = Queue()\n",
    "    inference_output_queues = {i: Queue() for i in range(num_model_servers)}\n",
    "    progress_queue = Queue()  \n",
    "    inference_proc = Process(target=inference_process, args=(inference_input_queue, inference_output_queues, lock,model_loaded_event))\n",
    "    inference_proc.start()\n",
    "    model_loaded_event.wait()\n",
    "    model_server_procs = []\n",
    "    for i in range(num_model_servers):\n",
    "        input_queue = Queue()  \n",
    "        output_queue = inference_output_queues[i] \n",
    "        model_proc = Process(target=model_server_task, args=(i,progress_queue,input_queue, output_queue, inference_input_queue, lock,model_loaded_event))\n",
    "        model_proc.start()\n",
    "        model_server_procs.append((model_proc, input_queue))\n",
    "\n",
    "   \n",
    "    input_folder = '/root/autodl-tmp/DATA/PRE_CUT'  \n",
    "    output_folder = '/root/autodl-tmp/DATA/Background_sound_delete' \n",
    "\n",
    "\n",
    "    if not os.path.exists(input_folder):\n",
    "        os.makedirs(input_folder)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    \n",
    "    wav_files = [f for f in os.listdir(input_folder) if f.endswith('.wav')]\n",
    "    \n",
    "   \n",
    "    pbar = tqdm(total=len(wav_files), desc=\"Processing WAV files\")\n",
    "    progress_process = Process(target=progress_updater, args=(progress_queue, pbar))\n",
    "    progress_process.start()\n",
    "    \n",
    "    for i, (model_proc, input_queue) in enumerate(model_server_procs):\n",
    "        for wav_file in wav_files[i::num_model_servers]:  # 分配策略\n",
    "            input_audio = os.path.join(input_folder, wav_file)\n",
    "            input_queue.put((input_audio, output_folder))\n",
    "    for _, input_queue in model_server_procs:\n",
    "        input_queue.put(None)\n",
    "\n",
    "    # 等待所有模型服务进程结束\n",
    "    for model_proc, _ in model_server_procs:\n",
    "        model_proc.join()\n",
    "\n",
    "    # 停止推理进程\n",
    "    inference_input_queue.put(None)\n",
    "    inference_proc.join()\n",
    "    progress_process.join()\n",
    "    \n",
    "    for _, input_queue in model_server_procs:\n",
    "        input_queue.close()\n",
    "    for _, output_queue in inference_output_queues.items():\n",
    "        output_queue.close()\n",
    "    inference_input_queue.close()\n",
    "    print(\"完毕\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5c056d-d558-4ee7-b169-e56e2f89cbf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "进行切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398a323-5abb-426c-a5ae-98d9ac563b30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "min_len=1\n",
    "max_len=20 #这里是最小2秒，最大20秒的设置\n",
    "input_directory = \"/root/autodl-tmp/DATA/Background_sound_delete\" \n",
    "output_directory = \"/root/autodl-tmp/DATA/Preprocessed\" \n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "def save_vocal_segments(args):\n",
    "    input_path, output_dir, min_len, max_len = args\n",
    "    audio = AudioSegment.from_wav(input_path)\n",
    "\n",
    "    chunks = split_on_silence(audio, \n",
    "                              min_silence_len=700,\n",
    "                              silence_thresh=-50)\n",
    "\n",
    "    for i, chunk in tqdm(enumerate(chunks), total=len(chunks), desc=f\"筛选中- 文件 {os.path.splitext(os.path.basename(input_path))[0]}\"):\n",
    "        chunk_length = len(chunk) / 1000\n",
    "        \n",
    "        if min_len <= chunk_length <= max_len:\n",
    "            output_file_name = os.path.splitext(os.path.basename(input_path))[0] + f\"_{i:03}.wav\"\n",
    "            output_path = os.path.join(output_dir, output_file_name)\n",
    "            chunk.export(output_path, format=\"wav\")\n",
    "\n",
    "\n",
    "files = [ filename for  filename in os.listdir(input_directory) if 'vocals'in filename]\n",
    "args = [(os.path.join(input_directory, f),output_directory ,min_len,max_len) for f in files]\n",
    "with Pool(None) as pool:\n",
    "    list(pool.imap(save_vocal_segments, args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcefdf-080d-4562-bb7c-8a03b690bcb5",
   "metadata": {
    "tags": []
   },
   "source": [
    "音频格式化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68943ad4-3284-40ca-9991-8afcb910e7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "音频格式化: 100%|██████████| 1627/1627 [00:02<00:00, 574.63it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "def audio_convert(audio,path):\n",
    "    audio = audio.set_channels(1).set_sample_width(2).set_frame_rate(44100)\n",
    "    audio.export(path, format='wav')\n",
    "\n",
    "input_directory = \"/root/autodl-tmp/DATA/Preprocessed\" \n",
    "\n",
    "with open('/root/autodl-tmp/DATA/Parameter_save.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    character_name = data[\"character_name\"]\n",
    "i = 1\n",
    "files = [f for f in os.listdir(input_directory)]\n",
    "for filename in tqdm( files, desc=\"音频格式化\"):\n",
    "    source_path = os.path.join(input_directory, filename)\n",
    "    if filename.endswith('.wav'):\n",
    "        target_filename = f\"{character_name}_{i:05d}.wav\"  # 重命名\n",
    "        target_path = os.path.join(input_directory, target_filename)\n",
    "        audio_convert(AudioSegment.from_file(source_path),target_path)\n",
    "        os.remove(source_path)\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f42d7-d62a-4b93-99e4-f63ef4365c17",
   "metadata": {
    "tags": []
   },
   "source": [
    "ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd85359d-ceec-4539-8c07-5770e6153caa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 查看是否有显存\n",
    "import logging\n",
    "import warnings\n",
    "import gc\n",
    "import logging\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "#由于multiprocessing库的特性，我们把处理函数打包到了 /root/tools/ASR_process.py 中\n",
    "from tools.ASR_process import process_file  \n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.multiprocessing\n",
    "import json\n",
    "\n",
    "with open('/root/autodl-tmp/DATA/Parameter_save.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    character_name = data[\"character_name\"]\n",
    "\n",
    "output_directory='autodl-tmp/DATA'\n",
    "input_directory = \"/root/autodl-tmp/DATA/Preprocessed\"\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger('modelscope').setLevel(logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('modelscope').setLevel(logging.CRITICAL)\n",
    "\n",
    "data = pd.DataFrame(columns=['file_path', 'text'])\n",
    "filelist = os.listdir(input_directory)\n",
    "with torch.multiprocessing.get_context(\"spawn\").Pool(8) as pool:\n",
    "    results = list(tqdm(pool.imap(process_file, filelist), total=len(filelist), desc=\"ASR\"))\n",
    "\n",
    "# 过滤掉返回None的结果\n",
    "results = [result for result in results if result is not None]\n",
    "\n",
    "# 创建DataFrame\n",
    "data = pd.DataFrame(results, columns=['file_path', 'text'])\n",
    "\n",
    "list_filename = os.path.join(output_directory, f\"{character_name}_raw.list\")\n",
    "\n",
    "with open(list_filename, 'w', encoding='utf-8') as list_file:\n",
    "    for _, row in data.iterrows():\n",
    "        #这里需使用绝对地址，避免后面出错\n",
    "        absolute_file_path = os.path.abspath(row['file_path'])\n",
    "        line = f\"{absolute_file_path}|{character_name}|{'ZH'}|{row['text']}\\n\"\n",
    "        list_file.write(line)\n",
    "        \n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53184d50-d95d-4719-9632-252056593c98",
   "metadata": {
    "tags": []
   },
   "source": [
    "制作对话集：这是经过摸索后权衡的方案，6b能力有限啊，你也来试试吧 :——） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753dac3a-6193-416d-bf07-56addf6b1b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 16:22:16.994820: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 16:22:17.807766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e396c47b0474e8a83201ecb269708ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c8c4dee67b4db7841ef61e9c03385f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/1351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 90\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tokenizer\n\u001b[1;32m     89\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 90\u001b[0m \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def get_prompt(text):\n",
    "    try:\n",
    "        with open('/root/autodl-tmp/DATA/Parameter_save.json', 'r') as file:\n",
    "            Parameter_save = json.load(file)\n",
    "    except Exception:\n",
    "        print(\"未找到配置文件，请检查：/root/autodl-tmp/DATA/Parameter_save.json文件是否存在\")\n",
    "        return\n",
    "    if \"character_setting\" in Parameter_save:\n",
    "        prompt= \"\"\"\n",
    "\n",
    "        请仔细分析，提取原句的关键信息和语气。你将扮演ccc，zzz作为回答方。模仿原句的风格，创建一个问答对话。问题应该简短直接，而回答则需要详细具体。\n",
    "\n",
    "        原句:xxxxx\n",
    "\n",
    "        基于原句内容，构造问答对话如下：\n",
    "\n",
    "        问题：（这里写出一个简短而直接的问题）\n",
    "        回答：(扮演ccc，尽可能回答得有情感，语气与原句相似)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        return prompt.replace('xxxxx', text).replace('ccc', Parameter_save[\"character_name_ZH\"]).replace('zzz', Parameter_save[\"character_setting\"])       \n",
    "    else:\n",
    "        \n",
    "        prompt = \"\"\"\n",
    "\n",
    "        请仔细分析，提取原句的关键信息和语气。你将扮演ccc。模仿原句的风格，创建一个问答对话。问题应该简短直接，而回答则需要详细具体。\n",
    "\n",
    "        原句:xxxxx\n",
    "\n",
    "        基于原句内容，构造问答对话如下：\n",
    "\n",
    "        问题：（这里写出一个简短而直接的问题）\n",
    "        回答：(扮演ccc，尽可能回答得有情感，语气与原句相似)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        return prompt.replace('xxxxx', text).replace('ccc', Parameter_save[\"character_name_ZH\"])\n",
    "\n",
    "\n",
    "def extract_question_answer(text):\n",
    "    question, answer = \"\", \"\"\n",
    "    if \"问题：\" in text:\n",
    "        question_part = text.split(\"问题：\")[1]\n",
    "        question = question_part.split(\"\\n\")[0] if \"\\n\" in question_part else question_part\n",
    "    if \"回答：\" in text:\n",
    "        answer_part = text.split(\"回答：\")[1]\n",
    "        answer = answer_part.split(\"\\n\")[0] if \"\\n\" in answer_part else answer_part\n",
    "    return question, answer\n",
    "\n",
    "with open('/root/autodl-tmp/DATA/Parameter_save.json', 'r') as file:\n",
    "    Parameter_save = json.load(file)\n",
    "model_name = '/root/autodl-tmp/chatglm3-6b'\n",
    "zh_character_name = Parameter_save['character_name_ZH']\n",
    "character_name=Parameter_save['character_name']\n",
    "file_path = f\"/root/autodl-tmp/DATA/{character_name}_raw.list\"\n",
    "data_path = \"/root/autodl-tmp/DATA/LLMdata.csv\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().cuda()\n",
    "\n",
    "qa_list = []\n",
    "total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file, tqdm(total=total_lines, desc=\"Processing\") as pbar:\n",
    "    for line in file:\n",
    "        pbar.update(1)\n",
    "        text = line.strip().split('|')[-1]\n",
    "        prompt = get_prompt( text)\n",
    "        text, _ = model.chat(tokenizer, prompt, history=[])\n",
    "        question, answer = extract_question_answer(text)\n",
    "        #一些基于经验的过滤\n",
    "        if \":\" in answer:\n",
    "             answer= answer.split(\":\")[-1].strip()\n",
    "        if \"：\" in answer:\n",
    "            answer= answer.split(\"：\")[-1].strip()\n",
    "        if question and answer and not \"助手\" in answer and not  \"语言模型\" in answer and not  \"AI\" in answer and not  \"人工智能\" in answer and not zh_character_name  in answer and len(answer)>5:\n",
    "\n",
    "            qa_list.append({'prompt': f\"{zh_character_name},{question}\", 'response': answer})\n",
    "\n",
    "pd.DataFrame(qa_list).to_csv(data_path)\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf441f-0d8f-4962-9d19-4f41b19d4b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "筛选音频文件（bertvits训练不需要太多音频）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "277ea9ee-5893-4779-b5c7-783bd27b91f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import json\n",
    "\n",
    "def process_wav_files(folder_path):\n",
    "    min_duration = 2  # 初始时长阈值为2秒\n",
    "\n",
    "    while True:\n",
    "        wav_files = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "        num_files = len(wav_files)\n",
    "\n",
    "        if num_files <= 500:\n",
    "            break\n",
    "\n",
    "        for wav_file in wav_files:\n",
    "            file_path = os.path.join(folder_path, wav_file)\n",
    "\n",
    "           \n",
    "            with wave.open(file_path, 'rb') as wf:\n",
    "                frames = wf.getnframes()\n",
    "                rate = wf.getframerate()\n",
    "                duration = frames / float(rate)\n",
    "                \n",
    "                if duration <= min_duration:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted {wav_file}\")\n",
    "\n",
    "        # 如果文件数量仍然大于500，增加时长阈值\n",
    "        min_duration += 1\n",
    "        if min_duration ==5:\n",
    "            break\n",
    "    with open('/root/autodl-tmp/DATA/Parameter_save.json', 'r') as file:\n",
    "        Parameter_save = json.load(file)\n",
    "    character_name = Parameter_save[\"character_name\"]\n",
    "    list_filename = os.path.join('/root/autodl-tmp/DATA/', f\"{character_name}_raw.list\")\n",
    "    with open(list_filename, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "  \n",
    "    valid_lines = []\n",
    "\n",
    "    \n",
    "    for line in lines:\n",
    "       \n",
    "        file_path = line.split('|')[0]\n",
    "\n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(file_path):\n",
    "            \n",
    "            valid_lines.append(line)\n",
    "\n",
    "    # 将有效的行写回\n",
    "    with open(os.path.join('/root/autodl-tmp/DATA/', f\"{character_name}.list\"), 'w', encoding='utf-8') as file:\n",
    "        file.writelines(valid_lines)\n",
    "\n",
    "input_path = '/root/autodl-tmp/DATA/Preprocessed' \n",
    "process_wav_files(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121a7af-148f-41d1-a597-074456b191a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
